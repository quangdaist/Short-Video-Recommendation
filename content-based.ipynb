{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyvi.ViTokenizer import ViTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read all file csv in raw folder\n",
    "path = r'raw'\n",
    "all_files = glob.glob(path + \"/*.csv\")\n",
    "history = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split attributes about video base on url\n",
    "for i in range(len(history)):\n",
    "    text = history.loc[i, 'url'].split('/')[-1]\n",
    "    info_video = re.findall(r'\\d+', text)\n",
    "    history.loc[i, 'id'] = info_video[0]  # id video\n",
    "    history.loc[i, 'from_webapp'] = info_video[1]  # from_webapp\n",
    "    history.loc[i, 'web_id'] = info_video[2]  # from_webapp\n",
    "\n",
    "history.drop_duplicates(subset='id', inplace=True)\n",
    "history.reset_index(inplace=True, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 643 entries, 0 to 642\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   url             643 non-null    object \n",
      " 1   desc_video      623 non-null    object \n",
      " 2   like_count      643 non-null    object \n",
      " 3   comment_count   643 non-null    object \n",
      " 4   like            643 non-null    int64  \n",
      " 5   time_container  643 non-null    object \n",
      " 6   timestamp       643 non-null    float64\n",
      " 7   user            643 non-null    object \n",
      " 8   id              643 non-null    object \n",
      " 9   from_webapp     643 non-null    object \n",
      " 10  web_id          643 non-null    object \n",
      "dtypes: float64(1), int64(1), object(9)\n",
      "memory usage: 55.4+ KB\n"
     ]
    }
   ],
   "source": [
    "history.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "history['desc_video'] = history['desc_video'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_stopwords():\n",
    "    \"\"\"\n",
    "    Get stopwords from GitHub\n",
    "    \"\"\"\n",
    "    stopwords_url = 'https://raw.githubusercontent.com/stopwords/vietnamese-stopwords/master/vietnamese-stopwords-dash.txt'\n",
    "    response = requests.get(stopwords_url)\n",
    "    stopwords = response.text.split('\\n')\n",
    "    stopwords = set(stopwords)\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "stopwords = download_stopwords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_stop_words(sentences, stop_words):\n",
    "    new_sent = [word for word in sentences.split() if word not in stop_words]\n",
    "    sentences = ' '.join(new_sent)\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def de_emojify(text):\n",
    "    regrex_pattern = re.compile(pattern=\"[\"\n",
    "                                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                        \"]+\", flags=re.UNICODE)\n",
    "    return regrex_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def preprocess(text, tokenized=True, lowercased=True):\n",
    "    text = text.lower() if lowercased else text\n",
    "    text = de_emojify(text)\n",
    "    text = ViTokenizer.tokenize(text) if tokenized else text\n",
    "    text = re.sub(r'[^\\w]', ' ', text)\n",
    "    text = filter_stop_words(text, stopwords)\n",
    "    return text\n",
    "\n",
    "\n",
    "def pre_process_features(sentences, tokenized=True, lowercased=True):\n",
    "    sentences = [preprocess(str(p), tokenized=tokenized,\n",
    "                            lowercased=lowercased) for p in list(sentences)]\n",
    "    for idx, ele in enumerate(sentences):\n",
    "        if not ele:\n",
    "            np.delete(sentences, idx)\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocess description\n",
    "descr_video = history['desc_video'].values\n",
    "descr_video_processed = pre_process_features(descr_video)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document frequency\n",
    "tfidf = TfidfVectorizer()\n",
    "descr_matrix = tfidf.fit_transform(descr_video_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# measure the similarity between 2 description vector base on Cosine_Similarity\n",
    "similarity_matrix = linear_kernel(descr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend 10 video are similarity for user watched video has id: \"7133533031737920811\"\n",
      "Video has id \"7166864341277396251\" with similarity: 0.35229262867658223\n",
      "Video has id \"7140522031476460826\" with similarity: 0.2902933145638706\n",
      "Video has id \"7164310529899629851\" with similarity: 0.24341507360668957\n",
      "Video has id \"7159574479822146817\" with similarity: 0.17350065195605596\n",
      "Video has id \"7142146157400132890\" with similarity: 0.14973074882596585\n",
      "Video has id \"7141371459607252251\" with similarity: 0.10018816041276869\n",
      "Video has id \"7166101487418871067\" with similarity: 0.0\n",
      "Video has id \"7160241510816976154\" with similarity: 0.0\n",
      "Video has id \"7161707438633127194\" with similarity: 0.0\n",
      "Video has id \"7161372876992466203\" with similarity: 0.0\n"
     ]
    }
   ],
   "source": [
    "def recommend_video_based_on_desc(video_id, num_video, similarity_matrix):\n",
    "    # get similarity values with other video\n",
    "    # similarity_score is the list of index and similarity matrix\n",
    "    vid_idx = history.index[history['id'] == video_id].to_list()[0]\n",
    "    similarity_score = list(enumerate(similarity_matrix[vid_idx]))\n",
    "    # sort in descending order the similarity score of movie inputted with all the other videos\n",
    "    similarity_score = sorted(\n",
    "        similarity_score, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores of the 15 most similar videos. Ignore the first movie.\n",
    "    similarity_score = similarity_score[1:num_video + 1]\n",
    "    # return movie names using the mapping series\n",
    "    video_indices = [(history.loc[idx[0], 'id'], idx[1])\n",
    "                     for idx in similarity_score]\n",
    "    print(f'Recommend {num_video} video are similarity for user watched video has id: \\\"{video_id}\\\"')\n",
    "    for i, v in video_indices:\n",
    "        print(f\"Video has id \\\"{i}\\\" with similarity: {v}\")\n",
    "\n",
    "\n",
    "video_id = history.loc[1, 'id']\n",
    "num_video = 10\n",
    "recommend_video_based_on_desc(video_id, num_video, similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Word to Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "# GitHub: https://test.ocom.vn/?url=github.com/sonvx/word2vecVN\n",
    "word2vec_path = 'word2vec_model/baomoi.window2.vn.model.bin.gz'\n",
    "w2v_model = models.KeyedVectors.load_word2vec_format(\n",
    "    word2vec_path, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of words: 439056\n"
     ]
    }
   ],
   "source": [
    "vocab = w2v_model.wv.vocab\n",
    "print(\"The total number of words:\", len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of key-value pairs: 439056\n"
     ]
    }
   ],
   "source": [
    "word_vec_dict = {}\n",
    "for word in vocab:\n",
    "    word_vec_dict[word] = w2v_model.wv.get_vector(word)\n",
    "print(\"The number of key-value pairs:\", len(word_vec_dict))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base on embedding vector (Developing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences have max 70 lengths\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "for val in descr_video:\n",
    "    len_sentence = len(str(val).split())\n",
    "    if len_sentence > max_length:\n",
    "        max_length = len_sentence\n",
    "print(\"Sentences have max %d lengths\" % max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 300  # dimension of a word vector\n",
    "enc_descr_processed = []\n",
    "for descr in descr_video_processed:\n",
    "    descr_list = descr.split()\n",
    "    w2v = []\n",
    "    for val in descr_list:\n",
    "        try:\n",
    "            _ = word_vec_dict[val]\n",
    "        except KeyError:\n",
    "            # if word isn't in vocab,\n",
    "            # assign it to zeros vector have 300 dim\n",
    "            _ = np.zeros((dim,))\n",
    "        w2v.append(_)\n",
    "    enc_descr_processed.append(w2v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(643, 70, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import pad_sequences\n",
    "# padding sentences to have equal length\n",
    "pad_cont = pad_sequences(enc_descr_processed, maxlen=max_length, dtype='float',\n",
    "                         padding='post', value=np.zeros(dim))\n",
    "pad_cont.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove word not in vocab, then calc similarity between 2 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "descr_in_vocab = []\n",
    "for descr in descr_video_processed:\n",
    "    descr_list = [_ for _ in descr.split() if _ in vocab]\n",
    "    descr_in_vocab.append(descr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = [[0]*len(history)]*len(history)\n",
    "for idx1, val1 in enumerate(descr_in_vocab):\n",
    "    for idx2, val2 in enumerate(descr_in_vocab):\n",
    "        try:\n",
    "            sim = w2v_model.wv.n_similarity(val1, val2)\n",
    "        except:\n",
    "            sim = 0\n",
    "        sim_matrix[idx1][idx2] = sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommend 10 video are similarity for user watched video has id: \"7133533031737920811\"\n",
      "Video has id \"7159525365185039642\" with similarity: 0.5890825986862183\n",
      "Video has id \"7140131471540489498\" with similarity: 0.5462914109230042\n",
      "Video has id \"7166590557802761499\" with similarity: 0.5410798192024231\n",
      "Video has id \"7151337992114818330\" with similarity: 0.5267125368118286\n",
      "Video has id \"7156818902658403611\" with similarity: 0.5265475511550903\n",
      "Video has id \"7165370368742411547\" with similarity: 0.5189292430877686\n",
      "Video has id \"7155123858289495342\" with similarity: 0.5100876688957214\n",
      "Video has id \"7165461787595377946\" with similarity: 0.49493736028671265\n",
      "Video has id \"7159542834763336986\" with similarity: 0.4862213432788849\n",
      "Video has id \"7155372076080041242\" with similarity: 0.47996985912323\n"
     ]
    }
   ],
   "source": [
    "def recommend_video_based_on_desc(video_id, num_video, similarity_matrix):\n",
    "    # get similarity values with other video\n",
    "    # similarity_score is the list of index and similarity matrix\n",
    "    vid_idx = history.index[history['id'] == video_id].to_list()[0]\n",
    "    similarity_score = list(enumerate(similarity_matrix[vid_idx]))\n",
    "    # sort in descending order the similarity score of movie inputted with all the other videos\n",
    "    similarity_score = sorted(\n",
    "        similarity_score, key=lambda x: x[1], reverse=True)\n",
    "    # Get the scores of the 15 most similar videos. Ignore the first movie.\n",
    "    similarity_score = similarity_score[1:num_video + 1]\n",
    "    # return movie names using the mapping series\n",
    "    video_indices = [(history.loc[idx[0], 'id'], idx[1])\n",
    "                     for idx in similarity_score]\n",
    "    print(f'Recommend {num_video} video are similarity for user watched video has id: \\\"{video_id}\\\"')\n",
    "    for i, v in video_indices:\n",
    "        print(f\"Video has id \\\"{i}\\\" with similarity: {v}\")\n",
    "\n",
    "\n",
    "video_id = history.loc[1, 'id']\n",
    "num_video = 10\n",
    "recommend_video_based_on_desc(video_id, num_video, sim_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6fff98fc3b3d81bd655c2cc48858186e4d9e2db7b515bf1c3221888f12a62f87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
